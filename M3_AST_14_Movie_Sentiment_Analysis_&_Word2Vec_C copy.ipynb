{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-14/blob/main/M3_AST_14_Movie_Sentiment_Analysis_%26_Word2Vec_C%20copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 14 : Implementing Movie Sentiment Analysis using LSTMs and Word2vec Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* build a Deep Neural Network for Sentiment Classification\n",
        "* learn Word Embedding : while training the network and using Word2Vec\n",
        "* understand cell structure of LSTMs\n",
        "* implement unidirectional and bidirectional LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ox5WyF0y4dMz"
      },
      "source": [
        "## Recurrent neural network (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7znelX-BRG1"
      },
      "source": [
        "RNN is a type of neural network that is proven to work well with sequence data. Since text is actually a sequence of words, a recurrent neural network is an automatic choice to solve text-related problems. Here, we will use an LSTM (Long Short Term Memory network) which is a variant of RNN, to solve a movie reviews based sentiment classification problem. \n",
        "\n",
        "An LSTM unit consists of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.\n",
        "\n",
        "LSTM networks are well-suited to classification based on time series data and deal well with the exploding and vanishing gradient problems that can be encountered when training traditional RNNs\n",
        "\n",
        "<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://miro.medium.com/max/1302/1*yr0820z7YNRcDCWGisLC4g.png\" width=\"450\" height=\"250\">\n",
        "\n",
        "\n",
        "<img style=\"-webkit-user-select: none;margin: auto;\" src=\"https://miro.medium.com/max/1276/1*mvxPFvnDqj2jJrsjevD41A.png\" width=\"450\" height=\"250\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4ch_va43O7"
      },
      "source": [
        "### Vanishing/exploding gradient \n",
        "* The vanishing and exploding gradient phenomena are often encountered in the context of RNNs. The reason why they happen is that it is difficult to capture long term dependencies because of multiplicative gradient that can be exponentially decreasing/increasing with respect to the number of layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jqekLXI5ewk"
      },
      "source": [
        "### LSTM\n",
        "\n",
        "\n",
        "*  Long Short-Term Memory units (LSTM) deal with the vanishing gradient problem encountered by traditional RNNs and able to remember a piece of information and keep it saved for many timesteps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25xeb09mMs0B"
      },
      "source": [
        "## Dataset Description\n",
        "\n",
        "The IMDB movie review dataset can be downloaded from [here](http://ai.stanford.edu/~amaas/data/sentiment/). This dataset for binary sentiment classification contains around 50K movie reviews with the following attributes:\n",
        "\n",
        "* **review:** review of any movie\n",
        "* **sentiment:** positive or negative sentiment value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWipVqzNLARK"
      },
      "source": [
        "### Sentiment Classification Problem\n",
        "\n",
        "Sentiment classification is the task of looking at a piece of text and telling if someone likes or dislikes the thing they’re talking about.\n",
        "The input X is a piece of text and the output Y is the sentiment which we want to predict.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_14_Movie_Sentiment_Analysis_&_Word2Vec_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/IMDB_Dataset.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RH8Ecq9sbYU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFl76_ngsasw"
      },
      "source": [
        "import pandas as pd # to load dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords # to get collection of stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# layers of the architecture\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.core import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Bidirectional\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer # to encode text to int\n",
        "from keras.models import Sequential   # the model\n",
        "from keras.utils import pad_sequences # to do padding or truncating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdT0aESgsVPU"
      },
      "source": [
        "### Load the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAzdHqulsRbv"
      },
      "source": [
        "movie_reviews = pd.read_csv(\"IMDB_Dataset.csv\")\n",
        "\n",
        "# Check for null values\n",
        "movie_reviews.isnull().values.any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r79To9Q4OGiI"
      },
      "source": [
        "print(movie_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJcLZO2aN_kl"
      },
      "source": [
        "# Print the first five rows from the data\n",
        "movie_reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXA3C6zsZ8fi"
      },
      "source": [
        "# Visualize the postive and negative sentiments\n",
        "movie_reviews.groupby(\"sentiment\").sentiment.count().plot.bar(ylim=0);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol2PIDSMtCwN"
      },
      "source": [
        "# Let us view one of the reviews\n",
        "movie_reviews[\"review\"][5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMIU5YtCv0k8"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Remove html tags, non alphabet (punctuations and numbers), stop words,  and lower case all of the reviews from the review text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8J-xAfCEQej"
      },
      "source": [
        "# Data Preprocessing \n",
        "def preprocess_text(sen):\n",
        "    \n",
        "    sen = re.sub('<.*?>', ' ', sen) # remove html tag\n",
        "\n",
        "    tokens = word_tokenize(sen)  # tokenizing words\n",
        "\n",
        "    tokens = [w.lower() for w in tokens]    # lower case\n",
        "\n",
        "    table = str.maketrans('', '', string.punctuation)  # remove punctuations\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "\n",
        "    words = [word for word in stripped if word.isalpha()]  # remove non alphabet\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = [w for w in words if not w in stop_words]   # remove stop words\n",
        "    words = [w for w in words if len(w) > 2]  # Ignore words less than 2\n",
        "    \n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjzvZSmwvO5O"
      },
      "source": [
        "# Store the preprocessed reviews in a new list\n",
        "review_lines = []\n",
        "sentences = list(movie_reviews['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G18YceU7mgAx"
      },
      "source": [
        "for sen in sentences:\n",
        "    # Call the preprocess_text function on each sentence of the review text \n",
        "    review_lines.append(preprocess_text(sen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqkMzMjrAe4V"
      },
      "source": [
        "# Check for the length of the preprocessed text\n",
        "len(review_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viX5aijFhlGf"
      },
      "source": [
        "# Print the preprocessed text for the first review\n",
        "print(review_lines[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-kFMtmcv6as"
      },
      "source": [
        "# Now let’s convert the sentiment from string to a binary form of 1 and 0, \n",
        "# where 1 is for ‘positive’ sentiment and 0 for ‘negative’.\n",
        "y = movie_reviews['sentiment']\n",
        "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ7P01yMVcai"
      },
      "source": [
        "### Word Embedding\n",
        "\n",
        "While dealing with textual data, we need to convert it into numbers before feeding into any machine learning model, including neural networks. For simplicity words can be compared to categorical variables. We use one-hot encoding to convert categorical features into numbers. To do so, we create dummy features for each of the category and populate them with 0’s and 1's.\n",
        "\n",
        "Similarly if we use one-hot encoding on words in textual data, we will have a dummy feature for each word, which means 10,000 features for a vocabulary of 10,000 words. This is not a feasible embedding approach as it demands large storage space for the word vectors and reduces model efficiency and no relation is captured between words.\n",
        "\n",
        "**Word2Vec** is one of the most popular technique to learn word embeddings using shallow neural network. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. A word embedding is a learned representation for text where words that have the same meaning have a similar representation.\n",
        "\n",
        "\n",
        "#### **Why do we need them?**\n",
        "\n",
        "Consider the following similar sentences: **Have a good day** and **Have a great day**. They hardly have different meaning. If we construct an vocabulary (let’s call it V), it would have V = **{Have, a, good, great, day}**.\n",
        "\n",
        "Now, let us create a one-hot encoded vector for each of these words in V. Length of our one-hot encoded vector would be equal to the size of V (=5). We would have a vector of zeros except for the element at the index representing the corresponding word in the vocabulary. That particular element would be one. The encodings below would explain this better.\n",
        "\n",
        "Have = [1,0,0,0,0] ; a = [0,1,0,0,0] ; good = [0,0,1,0,0] ; great = [0,0,0,1,0] ; day = [0,0,0,0,1]\n",
        "\n",
        "If we try to visualize these encodings, we can think of a 5 dimensional space, where each word occupies one of the dimensions and has nothing to do with the rest (no projection along the other dimensions). This means ‘good’ and ‘great’ are as different as ‘day’ and ‘have’, which is not true.\n",
        "\n",
        "Our objective is to have words with similar context occupy close spatial positions. Mathematically, the **cosine** of the angle between such vectors should be close to 1, i.e. angle close to 0. Higher the cosine similarity, the words are more closer\n",
        "\n",
        "\n",
        "**Cosie Similarity**\n",
        "\n",
        "$sim(A, B) = cos(\\theta) = \\frac{\\bar{A}. \\bar{B}}{\\bar{|A|}\\bar{|B|}}$\n",
        "\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Word_Embedding.png\" width=\"550\" height=\"350\">\n",
        "</center>\n",
        "\n",
        "**Word2vec** model has 2 algorithms:\n",
        "\n",
        "1. Continuous bag of word (CBOW)\n",
        "2. Skip-gram\n",
        "\n",
        "**Continuous bag of word (CBOW):**\n",
        "\n",
        "CBOW predicts the target words from the surrounding context words.\n",
        "\n",
        "**Eg: Context word:** \"The cat sits on the ..\",  **Target word:** \"mat\"\n",
        "\n",
        "**Skip-gram:**\n",
        "\n",
        "Skip-gram predicts surrounding context words from the target words\n",
        "\n",
        "**Eg: Context word:** \"The cat ... on the mat\",  **Target word:** \"sat\"\n",
        "\n",
        "**Note:** For more details of word2vec model refer to the following [link](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn42IGncfqFI"
      },
      "source": [
        "### Train word2vec Embedding\n",
        "\n",
        "Here we calculate the word vectors before hand to use it in the network. \n",
        "\n",
        "In Keras, this part is usually handled by an **Embedding layer** whuch enables us to convert each word into a fixed length vector of defined size. The resultant vector is a dense one with having real values instead of just 0’s and 1’s. The fixed length of word vectors helps us to represent words in a better way along with reduced dimensions. This way embedding layer works like a lookup table. The words are the keys in this table, while the dense word vectors are the values.\n",
        "\n",
        "Instead of training the embedding layer, we can first separately learn word embeddings and then pass to the embedding layer. \n",
        "\n",
        "We will use Gensim to  implement the Word2Vec. **Gensim** is an open source Python library for natural language processing. It is developed and is maintained by the Czech natural language processing researcher Radim Řehůřek and his company RaRe Technologies. Here, the first step is to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence.\n",
        "\n",
        "**Note:** Refer to the following [link](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec) for the parameters of gensim.models.Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY_cX3GeA3Z0"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Train word2vec model after preprocessing the reviews\n",
        "model = gensim.models.Word2Vec(sentences=review_lines, vector_size=EMBEDDING_DIM, window=5, workers=4, min_count=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "JbI2JhO5kEvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MELI-eY-Bt6q"
      },
      "source": [
        "words = list(model.wv.index_to_key)\n",
        "print('Vocabulary size: %d' % len(words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEovOd7rglXj"
      },
      "source": [
        "**sentences** – List of sentences; here we pass the list of review sentences.\n",
        "\n",
        "**size** – The number of dimensions in which we want to represent our word. This is the size of the word vector which instructs the Word2Vec() method to create a vector size of 100\n",
        "\n",
        "**min_count** – Word with frequency greater than min_count only are going to be included into the model. Usually, the bigger and more extensive your text, the higher this number can be.\n",
        "\n",
        "**window** – Only terms that occur within a window-neighborhood of a term, in a sentence, are associated with it during training. The usual value is 4 or 5.\n",
        "\n",
        "**workers** – Number of threads used in training parallelization, to speed up training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYQgWDH3hFp9"
      },
      "source": [
        "### Test Word2Vec Model\n",
        "\n",
        "After we train the model on our IMDb dataset, it builds a vocabulary size = 133617 . Let us try some word embeddings the model learnt from the movie review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ5ZbiLMhP6Q"
      },
      "source": [
        "The most similar words for word horrible are:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Weui1-5lEBRB"
      },
      "source": [
        "model.wv.most_similar('horrible')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F7AaZSQhcep"
      },
      "source": [
        "Analogy on the word vectors — woman + king - man = ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqcSSB6CIwIn"
      },
      "source": [
        "# Find the top-N most similar words, using the multiplicative combination \n",
        "# Let's see the result of semantically reasonable word vectors (king - man + woman)\n",
        "model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWEssgPnh2Qc"
      },
      "source": [
        "Let us find the odd word woman, king, queen, movie = ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4b51ORDL4gp"
      },
      "source": [
        "print(model.wv.doesnt_match(\"woman king queen movie\".split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU-Iqg5vh7nQ"
      },
      "source": [
        "It's interesting to see the word embeddings learned by our word2vec model form the text corpus. The next step is to use the word embeddings directly in the embedding layer in our sentiment classification model. we can save the model to be used later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdUrraXRJOy5"
      },
      "source": [
        "# Save model\n",
        "filename = \"imdb_embedding_word2vec.txt\"\n",
        "model.wv.save_word2vec_format(filename, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymsi9IR9OEKC"
      },
      "source": [
        "### Use Pre-trained Embedding Matrix\n",
        "\n",
        "Since we have already trained word2vec model with IMDb dataset, we have the word embeddings ready to use. The next step is to load the word embedding as a directory of words to vectors. The word embedding was saved in file imdb_embedding_word2vec.txt. Let us extract the word embeddings from the stored file. We defined the embedding matrix here, where all the words which are not in the word2vec dictionary being assigned a zero vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-3_L8R1OAaB"
      },
      "source": [
        "import os \n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join('','imdb_embedding_word2vec.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:])\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySTN_YiPFRu"
      },
      "source": [
        "The next step is to convert the word embedding into tokenized vector. The review documents should be integer encoded prior to passing them to the Embedding layer. The integer maps to the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the Embedding layer such that the encoded words map to the correct vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BbSPU936D0R"
      },
      "source": [
        "### Tokenize and Pad sequences\n",
        "\n",
        "A Neural Network only accepts numeric data, so we need to encode the reviews. Here use keras.Tokenizer() to encode the reviews into integers, where each unique word is automatically indexed (using `fit_on_texts` method) calculates the frequency of each word in our corpus/messages. \n",
        "\n",
        "`texts_to_sequences` method finally converts our array of sequences of strings to list of sequences of integers (most frequent word is assigned 1 and so on).\n",
        "\n",
        "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using `keras.preprocessing.sequence.pad_sequences.`\n",
        "\n",
        "`post`, pad or truncate the words in the back of a sentence\n",
        "`pre`, pad or truncate the words in front of a sentence\n",
        "\n",
        "Each word is assigned an integer and that integer is placed in a list. \n",
        "\n",
        "\n",
        "For example if we have a sentence “How text to sequence and padding works”. Each word is assigned a number. We suppose how = 1, text = 2, to = 3, sequence = 4, and = 5, padding = 6, works = 7. After texts_to_sequences is called our sentence will look like [1, 2, 3, 4, 5, 6, 7 ]. Now we suppose our MAX_SEQUENCE_LENGTH = 10. After padding our sentence will look like `pre` = [0, 0, 0, 1, 2, 3, 4, 5, 6, 7 ], `post` = [1, 2, 3, 4, 5, 6, 7, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtuLVKaSO8UK"
      },
      "source": [
        "# Tokenizer class from the keras.preprocessing.text module creates a word-to-index integer dictionary\n",
        "# Vectorize the text samples\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(review_lines)\n",
        "sequences = tokenizer.texts_to_sequences(review_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9rCOj07PmgE"
      },
      "source": [
        "# Pad sequences\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens' % len(word_index))\n",
        "\n",
        "# Only consider the first  100 words of each movie review\n",
        "max_length = 100\n",
        "\n",
        "review_pad = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "sentiment = y\n",
        "\n",
        "print('Shape of pad tensor:', review_pad.shape)\n",
        "print('Shape of sentiment tensor', sentiment.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7iUq1nGyOzx"
      },
      "source": [
        "The review_pad set contains 50,000 lists of integers, each list corresponding to the sentences in a review. Set the maximum length of each list to 100 and add 0 padding to those lists that have a length < 100, until they reach a length of 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS_AT_ob3jR4"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKpSTlPNNtvx"
      },
      "source": [
        "word_index attribute from Tokenizer class is a dictionary keeping track of word to their index/integer representation as calculated by fit_on_texts method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLU-hYnRjuLO"
      },
      "source": [
        "# Padded reviews\n",
        "print(review_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqTbw059jDNq"
      },
      "source": [
        "Now we will map embeddings from the loaded word2vec model for each word to the tokenizer.word_index vocabulary and create a matrix with of word vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzUftfDpQtOT"
      },
      "source": [
        "# Adding 1 because of reversed 0 index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Create a weight matrix for words in the training data\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "for word, index in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    # If words not found in embedding matrix will be all 0's\n",
        "    embedding_matrix[index, :] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLtW0A4KR61U"
      },
      "source": [
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DRqEIY6A3f"
      },
      "source": [
        "### Long Short-Term Memory units (LSTM Cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpQP6UCK1mzS"
      },
      "source": [
        "![Img](https://cdn.iisc.talentsprint.com/CDS/Images/LSTM_Cell.PNG)\n",
        "\n",
        "$\\text{Figure:  LSTM-Cell }$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsImcgfA0wed"
      },
      "source": [
        "### Understanding Gates and States\n",
        "\n",
        "\n",
        "**Forget gate** (Controlled by $f_{(t)}$) controls which parts of the long-term state should be erased.\n",
        "\n",
        "**Input gate** (controlled by :  $i_{(t)}$) controls which parts of a  $g_{(t)}$ should be added to the long term cell state  $c_{(t)}$.\n",
        "\n",
        "**Output gate** (controlled by $o_{(t)}$) controls which parts of the long term cell state should be read and output at this time step, both to prediction $y_{(t)}$  and $h_{(t)}$.\n",
        "\n",
        "**Long term Cell state**  $c_{(t)}$ is the \"memory\" that gets passed onto future time steps.\n",
        "\n",
        " **Short term Cell state** $h_{(t)}$\n",
        "\n",
        "* The Short term state gets passed to the LSTM cell's next time step.\n",
        "* It is used to determine the three gates (Controlled by $f_{(t)}$, $i_{(t)}$, $o_{(t)}$) of the next time step.\n",
        "* It is also used for the prediction $y_{(t)}$.\n",
        "\n",
        "**Embedding Layer:**  it creates word vectors of each word in the word_index and group words that are related or have similar meaning by analyzing other words around them.\n",
        "\n",
        "**LSTM Layer:** Deep network takes the sequence of embedding vectors as input and converts them to a compressed representation. The compressed representation effectively captures all the information in the sequence of words in the text.\n",
        "\n",
        "**Dense Layer:** The fully connected layer takes the deep representation from the LSTM and transforms it into the final output classes or class scores (using sigmoid activation function). \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6dm4lLnz_TE"
      },
      "source": [
        "#### Build LSTM Model for Text Classification\n",
        "\n",
        "We are now ready with the trained embedding vector to be used directly in the embedding layer. In the below code, we use the embedding_matrix as input to the Embedding layer and setting trainable = False, since the embedding is already learned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "338154WUTYNg"
      },
      "source": [
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Define Model\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM, \n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = max_length,\n",
        "                            trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(LSTM(32, dropout=0.3, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTMyl3NqXSfz"
      },
      "source": [
        "print('Summary of the built model...')\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuWHk7x0kQnU"
      },
      "source": [
        "# Try using different optimizers and different optimizer configs\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcXvriYXlAkF"
      },
      "source": [
        "To train the sentiment classification model, we use test_split= 0.2, you can vary this to see effect on the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRcgrWtgB4uz"
      },
      "source": [
        "### Split the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryFEyiCHVvVf"
      },
      "source": [
        "test_split = 0.2\n",
        "\n",
        "indices = np.arange(review_pad.shape[0])\n",
        "\n",
        "review_pad = review_pad[indices]\n",
        "sentiment = sentiment[indices]\n",
        "\n",
        "num_test_samples = int(test_split * review_pad.shape[0])\n",
        "\n",
        "X_train_pad = review_pad[:-num_test_samples]\n",
        "y_train = sentiment[:-num_test_samples]\n",
        "X_test_pad = review_pad[-num_test_samples:]\n",
        "y_test = sentiment[-num_test_samples:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAa70CQBX33b"
      },
      "source": [
        "X_train_pad.shape, y_train.shape, X_test_pad.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1yM4QUJ370N"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcRO-ixzTXYY"
      },
      "source": [
        "history = model.fit(X_train_pad, y_train, batch_size=128, verbose=1, epochs=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywm3gJT5mACU"
      },
      "source": [
        "### Visualization of Training and Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isKpLuqZlNS3"
      },
      "source": [
        "acc_train = history.history['accuracy']\n",
        "acc_val = history.history['val_accuracy']\n",
        "epochs = range(1,6)\n",
        "plt.plot(epochs, acc_train, 'g', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val, 'b', label='validation accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZPDdvqmP7V"
      },
      "source": [
        "### Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA4MFILaZxLX"
      },
      "source": [
        "print('Testing...')\n",
        "model.evaluate(X_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uam5tRLSrtH9"
      },
      "source": [
        "### Get the predictions using trained LSTM model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbvzPdAqa0hl"
      },
      "source": [
        "# model predictions on the test data\n",
        "preds = model.predict(X_test_pad)\n",
        "n = np.random.randint(0, 9999)\n",
        "\n",
        "# Predictions (set the threshold as 0.5)\n",
        "if preds[n] > 0.5:\n",
        "  print('predicted sentiment : positive')\n",
        "else: \n",
        "  print('precicted sentiment : negative')\n",
        "\n",
        "# Original Labels\n",
        "if (y_test[n] == 1):\n",
        "  print('correct sentiment : positive')\n",
        "else:\n",
        "  print('correct sentiment : negative')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlO8BAP7iTbq"
      },
      "source": [
        "# Get the text sequences for the preprocessed movie reviews\n",
        "reviews_list_idx = tokenizer.texts_to_sequences(review_lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuIsmiUXif0B"
      },
      "source": [
        "print(reviews_list_idx[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uO43uYffCfs"
      },
      "source": [
        "# Function to get the predictions on the movie reviews using LSTM model\n",
        "def add_score_predictions(data, reviews_list_idx):\n",
        "\n",
        "  # Pad the sequences of the data\n",
        "  reviews_list_idx = pad_sequences(reviews_list_idx, maxlen=max_length, padding='post', truncating='post')\n",
        "  \n",
        "  # Get the predictons by using LSTM model\n",
        "  review_preds = model.predict(reviews_list_idx)\n",
        "  \n",
        "  # Add the predictions to the movie reviews data\n",
        "  movie_reviews['sentiment score'] = review_preds\n",
        "  \n",
        "  # Set the threshold for the predictions\n",
        "  pred_sentiment = np.array(list(map(lambda x : 'positive' if x > 0.5 else 'negative',review_preds)))\n",
        "\n",
        "  # Add the sentiment predictions to the movie reviews\n",
        "  movie_reviews['predicted sentiment'] = pred_sentiment\n",
        "\n",
        "  return movie_reviews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pes81oz7hQpx"
      },
      "source": [
        "# Call the above function to get the sentiment score and the predicted sentiment\n",
        "data = add_score_predictions(movie_reviews, reviews_list_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtZbstE0i2cr"
      },
      "source": [
        "# Display the data\n",
        "data[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MtGdqE9DgSm"
      },
      "source": [
        "### BI-LSTM(Bi-directional long short term memory)\n",
        "\n",
        "Bidirectional long-short term memory(bi-lstm) is the process of making any neural network to have the sequence information in both directions backwards (future to past) or forward (past to future). \n",
        "\n",
        "In bidirectional, our input flows in two directions, making a bi-lstm different from the regular LSTM. With the regular LSTM, we can make input flow in one direction, either backwards or forward. However, in bi-directional, we can make the input flow in both directions to preserve the future and the past information.     \n",
        "\n",
        "For example: In the sentence “boys go to …..” we can not fill the blank space. Still, when we have a future sentence “boys come out of school”, we can easily predict the past blank space the similar thing we want to perform by our model and bidirectional LSTM allows the neural network to perform this.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/BI-LSTM.jpeg\" width=\"500\" height=\"350\">\n",
        "</center>\n",
        "\n",
        "In the above diagram, we can see the flow of information from backward and forward layers. BI-LSTM is usually employed where the sequence to sequence tasks are needed. This kind of network can be used in text classification, speech recognition and forecasting "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vKmEZzTmwAT"
      },
      "source": [
        "### Train Bi-directional LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYMwMyzemvE7"
      },
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "# Define Model\n",
        "embedding_layer = Embedding(vocab_size,\n",
        "                            EMBEDDING_DIM, \n",
        "                            weights = [embedding_matrix],\n",
        "                            input_length = max_length,\n",
        "                            trainable=False)\n",
        "model2.add(embedding_layer)\n",
        "model2.add(Bidirectional(LSTM(64, recurrent_dropout=0.6)))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmWir456MUnG"
      },
      "source": [
        "print('Summary of the built model...')\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEZYYfpmvi93"
      },
      "source": [
        "# Train the Bidirectional LSTM model\n",
        "history2 = model2.fit(X_train_pad, y_train, batch_size=128, verbose=1, epochs=5, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adxs9OHb5MJJ"
      },
      "source": [
        "# Evaluate the trained model on test data\n",
        "print('Testing...')\n",
        "model2.evaluate(X_test_pad, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPCjwxM25O--"
      },
      "source": [
        "# Let us test some samples using Bi-directional LSTMs\n",
        "test_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "test_sample_2 = \"Good movie!\"\n",
        "test_sample_3 = \"Maybe I like this movie.\"\n",
        "test_sample_4 = \"Not to my taste, will skip and watch another movie\"\n",
        "test_sample_5 = \"if you don't like action, then this movie might be bad for you.\"\n",
        "test_sample_6 = \"Bad movie!\"\n",
        "test_sample_7 = \"This movie really sucks! Can I get my money back please?\"\n",
        "test_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7]\n",
        "\n",
        "for each in test_samples:\n",
        "  filtered = [preprocess_text(each)]\n",
        "  tokenize_words = tokenizer.texts_to_sequences(filtered)\n",
        "  tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
        "  result = model2.predict(tokenize_words)\n",
        "\n",
        "  if result >= 0.5:\n",
        "      print('positive == ',each)\n",
        "  else:\n",
        "      print('negative == ',each)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following sentences and answer Q1.\n",
        "\n",
        "s1 = \"This is a foo bar sentence .\"\n",
        "\n",
        "s2 = \"This sentence is similar to a foo bar sentence .\"\n",
        "\n",
        "**Note:** You can create a similarity measure between the documents by using cosine similarity formula or python implementation\n",
        "\n",
        "**Hint:** You can use the **term frequency** for the words in above sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Find the cosine similarity between the above two sentences by including stopwords? So, please do NOT ignore stopwords while computing the cosine similarity between the two given sentences above.\n",
        "Answer1 = \"0.8616\" #@param [\"\",\"0.8616\",\"1.7232\",\"0.4308\",\"0.6092\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhXhSrKz6rva"
      },
      "source": [
        "#### Consider the following Statements and answer Q2.\n",
        "\n",
        "A. Batch Normalization\n",
        "\n",
        "B. Using ReLU instead of Sigmoid\n",
        "\n",
        "C. Using LSTMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. From the above options which helps to alleviate the Vanishing Gradient problem.\n",
        "Answer2 = \"Both B and C\" #@param [\"\",\"Both A and B\", \"Both B and C\", \"Both A and C\", \"All A, B, C of the above \"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}